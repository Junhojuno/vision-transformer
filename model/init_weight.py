import torch
import torch.nn as nn
import math
import warnings

from torch.functional import norm


def trunc_normal(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):
    """
    Fills the input tensor with`values drawn from a truncated normal distribution.
    The values are effectively drawn from the normal distribution(`N(mu, var)`) 
    with values outside(`[a, b]`) redrawn until they are within the bounds.
    The method used for generating the random values works best when `a <= mean <= b`.

    Args:
        tensor (torch.Tensor): an n-dimensional tensor
        mean (float): the mean of normal distribution
        std (float): the standard deviation of the normal distribution
        a (float): the minimum cutoff value
        b (float): the maximum cutoff value

    Returns:
        [torch.Tensor]: an n-dimensional tensor which is truncated
    """
    def norm_cdf(x):
        """Computes the standard normal cumulative distribution function"""
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using truncated uniform distribution
        # and then using the inverse CDF for the normal distribution.
        # Get upper and lower CDF values
        lower = norm_cdf((a - mean) / std)
        upper = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with the values from [lower, upper]
        # and then translate to [2*lower-1, 2*upper-1].
        tensor.uniform_(2 * lower - 1, 2 * upper - 1)

        # Use inverse CDF transform for normal distribution
        # to get truncated standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)

    return tensor

""" ViT weight initialization
    * When called without n, head_bias, jax_impl args it will behave exactly the same
      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).
    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl
    """
def init_vit_weights(module, name='', head_bias= 0., jax_impl=False):
    """ ViT weight initialization
    * When called without n, head_bias, jax_impl args it will behave exactly the same
      as my original init for compatibility with prev hparam / downstream use cases (ie DeiT).
    * When called w/ valid n (module name) and jax_impl=True, will (hopefully) match JAX impl
    

    Args:
        module (nn.Module): layer module
        name (str, optional): [description]. Defaults to ''.
        head_bias ([type], optional): [description]. Defaults to 0..
        jax_impl (bool, optional): [description]. Defaults to False.
    """
    if isinstance(module, nn.Linear):
        if name.startswith('head'):
            nn.init.zeros_(module.weight)
            nn.init.constant_(module.bias, head_bias)
        elif name.startswith('pre_logits'):
            module.weight = lecun_normal(module.weight)
            nn.init.zeros_(module.bias)
        else:
            if jax_impl:
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    if 'mlp' in name:
                        nn.init.normal_(module.bias, std=1e-6)
                    else:
                        nn.init.zeros_(module.bias)
            else:
                module.weight = trunc_normal(module.weight, std=.02)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
    elif jax_impl and isinstance(module, nn.Conv2d):
        # NOTE conv was left to pytorch default in my original init
        module.weight = lecun_normal(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif isinstance(module, (nn.LayerNorm, nn.GroupNorm, nn.BatchNorm2d)):
        nn.init.zeros_(module.bias)
        nn.init.ones_(module.weight)


def lecun_normal(tensor, scale=1.0, mode='fan_in', distribution='normal'):
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)

    # variance scaling
    if mode == 'fan_in':
        denominator = fan_in
    elif mode == 'fan_out':
        denominator = fan_out
    else: # fan_avg
        denominator = (fan_in + fan_out) / 2

    variance = scale / denominator

    if distribution == 'truncated_normal':
        std = math.sqrt(variance) / .87962566103423978
        tensor = trunc_normal(tensor, std=std)  
    elif distribution == 'normal':
        tensor.normal_(std=math.sqrt(variance))
    elif distribution == 'uniform':
        bound = math.sqrt(3 * variance)
        tensor.uniform_(-bound, bound)
    else:
        raise ValueError(f"invalid distribution {distribution}")
    
    return tensor


def _calculate_fan_in_and_fan_out(tensor):
    dimensions = tensor.dim()
    if dimensions < 2:
        raise ValueError("Fan in and fan out can not be computed for tensor with fewer than 2 dimensions")

    num_input_fmaps = tensor.size(1)
    num_output_fmaps = tensor.size(0)
    receptive_field_size = 1
    if tensor.dim() > 2:
        # math.prod is not always available, accumulate the product manually
        # we could use functools.reduce but that is not supported by TorchScript
        for s in tensor.shape[2:]:
            receptive_field_size *= s
    fan_in = num_input_fmaps * receptive_field_size
    fan_out = num_output_fmaps * receptive_field_size

    return fan_in, fan_out
